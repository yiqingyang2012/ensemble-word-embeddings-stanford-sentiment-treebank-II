{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, CuDNN not available)\n",
      "c:\\scisoft\\theano\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import theano\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano.tensor as T\n",
    "from pandas import DataFrame\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame.from_csv('all_train1.csv', index_col=False)\n",
    "valid_ = pd.DataFrame.from_csv('valid.csv'  , index_col=False)\n",
    "test_  = pd.DataFrame.from_csv('test.csv'   , index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('X_hidden.data', 'r') as f:\n",
    "    X_hidden = pickle.load(f)\n",
    "with open('index.data', 'r') as f:\n",
    "    index = pickle.load(f)\n",
    "with open('mapping.data', 'r') as f:\n",
    "    mapping = pickle.load(f) \n",
    "with open('clean_train_reviews.data', 'r') as f:\n",
    "    clean_train_reviews = pickle.load(f)\n",
    "with open('clean_valid_reviews.data', 'r') as f:\n",
    "    clean_valid_reviews = pickle.load(f)\n",
    "with open('clean_test_reviews.data', 'r') as f:\n",
    "    clean_test_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create feature vectors for documents using word-vector averages\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_voc_50 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_50.append(mapping[index[0][i]])\n",
    "table50 = [item for item in X_hidden[0]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table50, columns=headers)\n",
    "df.insert(0, 'word', new_voc_50)\n",
    "np.savetxt(r'ensemble_vectors_epoch50.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_100 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_100.append(mapping[index[1][i]])\n",
    "table100 = [item for item in X_hidden[1]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table100, columns=headers)\n",
    "df.insert(0, 'word', new_voc_100)\n",
    "np.savetxt(r'ensemble_vectors_epoch100.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_150 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_150.append(mapping[index[2][i]])\n",
    "table150 = [item for item in X_hidden[2]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table150, columns=headers)\n",
    "df.insert(0, 'word', new_voc_150)\n",
    "np.savetxt(r'ensemble_vectors_epoch150.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_200 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_200.append(mapping[index[3][i]])\n",
    "table200 = [item for item in X_hidden[3]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table200, columns=headers)\n",
    "df.insert(0, 'word', new_voc_200)\n",
    "np.savetxt(r'ensemble_vectors_epoch200.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_250 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_250.append(mapping[index[4][i]])\n",
    "table250 = [item for item in X_hidden[4]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table250, columns=headers)\n",
    "df.insert(0, 'word', new_voc_250)\n",
    "np.savetxt(r'ensemble_vectors_epoch250.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_300 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_300.append(mapping[index[5][i]])\n",
    "table300 = [item for item in X_hidden[5]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table300, columns=headers)\n",
    "df.insert(0, 'word', new_voc_300)\n",
    "np.savetxt(r'ensemble_vectors_epoch300.txt', df.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "gensim_file50='ensemble_vectors_epoch50.txt'\n",
    "model50 = gensim.models.Word2Vec.load_word2vec_format(gensim_file50, binary=False) \n",
    "model50.init_sims(replace=True)\n",
    "ensemble_train50 = getAvgFeatureVecs(clean_train_reviews, model50, 300)\n",
    "X_train50 = ensemble_train50.astype('float32')\n",
    "ensemble_test50  = getAvgFeatureVecs(clean_test_reviews, model50, 300)\n",
    "X_test50 = ensemble_test50.astype('float32')\n",
    "\n",
    "gensim_file100='ensemble_vectors_epoch100.txt'\n",
    "model100 = gensim.models.Word2Vec.load_word2vec_format(gensim_file100, binary=False) \n",
    "model100.init_sims(replace=True)\n",
    "ensemble_train100 = getAvgFeatureVecs(clean_train_reviews, model100, 300)\n",
    "X_train100 = ensemble_train100.astype('float32')\n",
    "ensemble_test100  = getAvgFeatureVecs(clean_test_reviews, model100, 300)\n",
    "X_test100 = ensemble_test100.astype('float32')\n",
    "\n",
    "gensim_file150='ensemble_vectors_epoch150.txt'\n",
    "model150 = gensim.models.Word2Vec.load_word2vec_format(gensim_file150, binary=False) \n",
    "model150.init_sims(replace=True)\n",
    "ensemble_train150 = getAvgFeatureVecs(clean_train_reviews, model150, 300)\n",
    "X_train150 = ensemble_train150.astype('float32')\n",
    "ensemble_test150  = getAvgFeatureVecs(clean_test_reviews, model150, 300)\n",
    "X_test150 = ensemble_test150.astype('float32')\n",
    "\n",
    "gensim_file200='ensemble_vectors_epoch200.txt'\n",
    "model200 = gensim.models.Word2Vec.load_word2vec_format(gensim_file200, binary=False) \n",
    "model200.init_sims(replace=True)\n",
    "ensemble_train200 = getAvgFeatureVecs(clean_train_reviews, model200, 300)\n",
    "X_train200 = ensemble_train200.astype('float32')\n",
    "ensemble_test200  = getAvgFeatureVecs(clean_test_reviews, model200, 300)\n",
    "X_test200 = ensemble_test200.astype('float32')\n",
    "\n",
    "gensim_file250='ensemble_vectors_epoch250.txt'\n",
    "model250 = gensim.models.Word2Vec.load_word2vec_format(gensim_file250, binary=False) \n",
    "model250.init_sims(replace=True)\n",
    "ensemble_train250 = getAvgFeatureVecs(clean_train_reviews, model250, 300)\n",
    "X_train250 = ensemble_train250.astype('float32')\n",
    "ensemble_test250  = getAvgFeatureVecs(clean_test_reviews, model250, 300)\n",
    "X_test250 = ensemble_test250.astype('float32')\n",
    "\n",
    "gensim_file300='ensemble_vectors_epoch300.txt'\n",
    "model300 = gensim.models.Word2Vec.load_word2vec_format(gensim_file300, binary=False) \n",
    "model300.init_sims(replace=True)\n",
    "ensemble_train300 = getAvgFeatureVecs(clean_train_reviews, model300, 300)\n",
    "X_train300 = ensemble_train300.astype('float32')\n",
    "ensemble_test300  = getAvgFeatureVecs(clean_test_reviews, model300, 300)\n",
    "X_test300 = ensemble_test300.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814387699066\n",
      "0.813289401428\n",
      "0.81164195497\n",
      "0.810543657331\n",
      "0.810543657331\n",
      "0.809994508512\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train50, train_['label'])\n",
    "print clf.score(X_test50, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train100, train_['label'])\n",
    "print clf.score(X_test100, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train150, train_['label'])\n",
    "print clf.score(X_test150, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train200, train_['label'])\n",
    "print clf.score(X_test200, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train250, train_['label'])\n",
    "print clf.score(X_test250, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train300, train_['label'])\n",
    "print clf.score(X_test300, test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "X_valid50  = getAvgFeatureVecs(clean_valid_reviews, model50 , 300)\n",
    "X_valid100 = getAvgFeatureVecs(clean_valid_reviews, model100, 300)\n",
    "X_valid150 = getAvgFeatureVecs(clean_valid_reviews, model150, 300)\n",
    "X_valid200 = getAvgFeatureVecs(clean_valid_reviews, model200, 300)\n",
    "X_valid250 = getAvgFeatureVecs(clean_valid_reviews, model250, 300)\n",
    "X_valid300 = getAvgFeatureVecs(clean_valid_reviews, model300, 300)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(train_['label'])\n",
    "y_valid_ohe = np_utils.to_categorical(valid_['label'])\n",
    "y_test_ohe  = np_utils.to_categorical(test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1764 - acc: 0.7296 - val_loss: 0.1446 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1364 - acc: 0.8061 - val_loss: 0.1383 - val_acc: 0.8028\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1334 - acc: 0.8111 - val_loss: 0.1374 - val_acc: 0.8142\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1326 - acc: 0.8125 - val_loss: 0.1406 - val_acc: 0.8039\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1313 - acc: 0.8135 - val_loss: 0.1360 - val_acc: 0.7947\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1300 - acc: 0.8164 - val_loss: 0.1355 - val_acc: 0.8028\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1292 - acc: 0.8164 - val_loss: 0.1351 - val_acc: 0.8005\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1285 - acc: 0.8190 - val_loss: 0.1349 - val_acc: 0.7982\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1275 - acc: 0.8205 - val_loss: 0.1346 - val_acc: 0.7993\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1270 - acc: 0.8216 - val_loss: 0.1350 - val_acc: 0.7982\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1254 - acc: 0.8231 - val_loss: 0.1354 - val_acc: 0.8096\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1243 - acc: 0.8247 - val_loss: 0.1343 - val_acc: 0.8050\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1232 - acc: 0.8270 - val_loss: 0.1371 - val_acc: 0.8005\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1216 - acc: 0.8297 - val_loss: 0.1357 - val_acc: 0.8096\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1208 - acc: 0.8307 - val_loss: 0.1337 - val_acc: 0.8062\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.809994508512\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train50,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid50,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test50))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1738 - acc: 0.7339 - val_loss: 0.1433 - val_acc: 0.7890\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1352 - acc: 0.8078 - val_loss: 0.1367 - val_acc: 0.7982\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1321 - acc: 0.8125 - val_loss: 0.1357 - val_acc: 0.8016\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1311 - acc: 0.8146 - val_loss: 0.1398 - val_acc: 0.8073\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1298 - acc: 0.8159 - val_loss: 0.1346 - val_acc: 0.7959\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1284 - acc: 0.8199 - val_loss: 0.1344 - val_acc: 0.8050\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1277 - acc: 0.8198 - val_loss: 0.1347 - val_acc: 0.8108\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1271 - acc: 0.8216 - val_loss: 0.1338 - val_acc: 0.7947\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1264 - acc: 0.8231 - val_loss: 0.1335 - val_acc: 0.8073\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1258 - acc: 0.8238 - val_loss: 0.1336 - val_acc: 0.8154\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1246 - acc: 0.8248 - val_loss: 0.1357 - val_acc: 0.8119\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1236 - acc: 0.8269 - val_loss: 0.1326 - val_acc: 0.8073\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1231 - acc: 0.8288 - val_loss: 0.1344 - val_acc: 0.8028\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1218 - acc: 0.8304 - val_loss: 0.1349 - val_acc: 0.8073\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1214 - acc: 0.8306 - val_loss: 0.1318 - val_acc: 0.8073\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.81164195497\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train100,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid100,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test100))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1708 - acc: 0.7423 - val_loss: 0.1429 - val_acc: 0.7913\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1350 - acc: 0.8083 - val_loss: 0.1371 - val_acc: 0.7936\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1323 - acc: 0.8123 - val_loss: 0.1366 - val_acc: 0.8050\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1317 - acc: 0.8129 - val_loss: 0.1390 - val_acc: 0.8039\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1305 - acc: 0.8144 - val_loss: 0.1354 - val_acc: 0.7924\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1289 - acc: 0.8182 - val_loss: 0.1345 - val_acc: 0.8016\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1277 - acc: 0.8206 - val_loss: 0.1344 - val_acc: 0.8039\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1267 - acc: 0.8218 - val_loss: 0.1343 - val_acc: 0.7936\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1254 - acc: 0.8236 - val_loss: 0.1343 - val_acc: 0.8005\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1242 - acc: 0.8255 - val_loss: 0.1348 - val_acc: 0.8085\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1226 - acc: 0.8280 - val_loss: 0.1351 - val_acc: 0.8016\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1212 - acc: 0.8308 - val_loss: 0.1339 - val_acc: 0.8085\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1203 - acc: 0.8330 - val_loss: 0.1377 - val_acc: 0.7924\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1187 - acc: 0.8350 - val_loss: 0.1355 - val_acc: 0.8028\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1177 - acc: 0.8359 - val_loss: 0.1337 - val_acc: 0.8096\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.806699615596\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train150,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid150,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test150))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1704 - acc: 0.7434 - val_loss: 0.1424 - val_acc: 0.7924\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1349 - acc: 0.8087 - val_loss: 0.1372 - val_acc: 0.7970\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1322 - acc: 0.8121 - val_loss: 0.1363 - val_acc: 0.8073\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1313 - acc: 0.8138 - val_loss: 0.1389 - val_acc: 0.7982\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1298 - acc: 0.8156 - val_loss: 0.1349 - val_acc: 0.7993\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1281 - acc: 0.8199 - val_loss: 0.1338 - val_acc: 0.7970\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1270 - acc: 0.8218 - val_loss: 0.1340 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1262 - acc: 0.8225 - val_loss: 0.1339 - val_acc: 0.7970\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1250 - acc: 0.8245 - val_loss: 0.1326 - val_acc: 0.8005\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1238 - acc: 0.8262 - val_loss: 0.1339 - val_acc: 0.8085\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1223 - acc: 0.8279 - val_loss: 0.1345 - val_acc: 0.8119\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1209 - acc: 0.8303 - val_loss: 0.1318 - val_acc: 0.8073\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1198 - acc: 0.8326 - val_loss: 0.1333 - val_acc: 0.8016\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1183 - acc: 0.8358 - val_loss: 0.1344 - val_acc: 0.8108\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1170 - acc: 0.8377 - val_loss: 0.1308 - val_acc: 0.8085\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.807797913234\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train200,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid200,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test200))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1685 - acc: 0.7467 - val_loss: 0.1420 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1347 - acc: 0.8093 - val_loss: 0.1367 - val_acc: 0.7982\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1317 - acc: 0.8137 - val_loss: 0.1357 - val_acc: 0.8028\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1305 - acc: 0.8154 - val_loss: 0.1389 - val_acc: 0.7936\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1287 - acc: 0.8183 - val_loss: 0.1343 - val_acc: 0.7982\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1266 - acc: 0.8216 - val_loss: 0.1338 - val_acc: 0.8028\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1250 - acc: 0.8249 - val_loss: 0.1332 - val_acc: 0.8050\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1237 - acc: 0.8268 - val_loss: 0.1324 - val_acc: 0.8062\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1219 - acc: 0.8304 - val_loss: 0.1326 - val_acc: 0.8028\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1205 - acc: 0.8324 - val_loss: 0.1333 - val_acc: 0.8096\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1187 - acc: 0.8346 - val_loss: 0.1342 - val_acc: 0.8142\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1168 - acc: 0.8376 - val_loss: 0.1319 - val_acc: 0.8119\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1154 - acc: 0.8402 - val_loss: 0.1332 - val_acc: 0.8039\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1135 - acc: 0.8448 - val_loss: 0.1347 - val_acc: 0.8062\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1122 - acc: 0.8462 - val_loss: 0.1315 - val_acc: 0.8142\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.81109280615\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train250,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid250,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test250))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1670 - acc: 0.7509 - val_loss: 0.1411 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1343 - acc: 0.8097 - val_loss: 0.1357 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1314 - acc: 0.8147 - val_loss: 0.1350 - val_acc: 0.8039\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1302 - acc: 0.8158 - val_loss: 0.1374 - val_acc: 0.8016\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1283 - acc: 0.8186 - val_loss: 0.1347 - val_acc: 0.8005\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1263 - acc: 0.8226 - val_loss: 0.1335 - val_acc: 0.8073\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1249 - acc: 0.8233 - val_loss: 0.1339 - val_acc: 0.8039\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1239 - acc: 0.8248 - val_loss: 0.1334 - val_acc: 0.8096\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1225 - acc: 0.8281 - val_loss: 0.1341 - val_acc: 0.8108\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1212 - acc: 0.8297 - val_loss: 0.1340 - val_acc: 0.8108\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1196 - acc: 0.8329 - val_loss: 0.1340 - val_acc: 0.8154\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1180 - acc: 0.8364 - val_loss: 0.1343 - val_acc: 0.8096\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1164 - acc: 0.8393 - val_loss: 0.1366 - val_acc: 0.8005\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1142 - acc: 0.8433 - val_loss: 0.1376 - val_acc: 0.7959\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1123 - acc: 0.8467 - val_loss: 0.1338 - val_acc: 0.8016\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.807248764415\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train300,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid300,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test300))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
