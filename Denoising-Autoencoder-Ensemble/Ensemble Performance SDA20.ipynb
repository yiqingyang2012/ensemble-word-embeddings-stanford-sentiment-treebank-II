{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, CuDNN not available)\n",
      "c:\\scisoft\\theano\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import theano\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano.tensor as T\n",
    "from pandas import DataFrame\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame.from_csv('all_train1.csv', index_col=False)\n",
    "valid_ = pd.DataFrame.from_csv('valid.csv'  , index_col=False)\n",
    "test_  = pd.DataFrame.from_csv('test.csv'   , index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('X_hidden20.data', 'r') as f:\n",
    "    X_hidden = pickle.load(f)\n",
    "with open('index20.data', 'r') as f:\n",
    "    index = pickle.load(f)\n",
    "with open('mapping.data', 'r') as f:\n",
    "    mapping = pickle.load(f) \n",
    "with open('clean_train_reviews.data', 'r') as f:\n",
    "    clean_train_reviews = pickle.load(f)\n",
    "with open('clean_valid_reviews.data', 'r') as f:\n",
    "    clean_valid_reviews = pickle.load(f)\n",
    "with open('clean_test_reviews.data', 'r') as f:\n",
    "    clean_test_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create feature vectors for documents using word-vector averages\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_voc_50 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_50.append(mapping[index[0][i]])\n",
    "table50 = [item for item in X_hidden[0]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table50, columns=headers)\n",
    "df.insert(0, 'word', new_voc_50)\n",
    "np.savetxt(r'20ensemble_vectors_epoch50.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_100 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_100.append(mapping[index[1][i]])\n",
    "table100 = [item for item in X_hidden[1]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table100, columns=headers)\n",
    "df.insert(0, 'word', new_voc_100)\n",
    "np.savetxt(r'20ensemble_vectors_epoch100.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_150 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_150.append(mapping[index[2][i]])\n",
    "table150 = [item for item in X_hidden[2]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table150, columns=headers)\n",
    "df.insert(0, 'word', new_voc_150)\n",
    "np.savetxt(r'20ensemble_vectors_epoch150.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_200 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_200.append(mapping[index[3][i]])\n",
    "table200 = [item for item in X_hidden[3]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table200, columns=headers)\n",
    "df.insert(0, 'word', new_voc_200)\n",
    "np.savetxt(r'20ensemble_vectors_epoch200.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_250 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_250.append(mapping[index[4][i]])\n",
    "table250 = [item for item in X_hidden[4]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table250, columns=headers)\n",
    "df.insert(0, 'word', new_voc_250)\n",
    "np.savetxt(r'20ensemble_vectors_epoch250.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_300 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_300.append(mapping[index[5][i]])\n",
    "table300 = [item for item in X_hidden[5]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table300, columns=headers)\n",
    "df.insert(0, 'word', new_voc_300)\n",
    "np.savetxt(r'20ensemble_vectors_epoch300.txt', df.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "gensim_file50='20ensemble_vectors_epoch50.txt'\n",
    "model50 = gensim.models.Word2Vec.load_word2vec_format(gensim_file50, binary=False) \n",
    "model50.init_sims(replace=True)\n",
    "ensemble_train50 = getAvgFeatureVecs(clean_train_reviews, model50, 300)\n",
    "X_train50 = ensemble_train50.astype('float32')\n",
    "ensemble_test50  = getAvgFeatureVecs(clean_test_reviews, model50, 300)\n",
    "X_test50 = ensemble_test50.astype('float32')\n",
    "\n",
    "gensim_file100='20ensemble_vectors_epoch100.txt'\n",
    "model100 = gensim.models.Word2Vec.load_word2vec_format(gensim_file100, binary=False) \n",
    "model100.init_sims(replace=True)\n",
    "ensemble_train100 = getAvgFeatureVecs(clean_train_reviews, model100, 300)\n",
    "X_train100 = ensemble_train100.astype('float32')\n",
    "ensemble_test100  = getAvgFeatureVecs(clean_test_reviews, model100, 300)\n",
    "X_test100 = ensemble_test100.astype('float32')\n",
    "\n",
    "gensim_file150='20ensemble_vectors_epoch150.txt'\n",
    "model150 = gensim.models.Word2Vec.load_word2vec_format(gensim_file150, binary=False) \n",
    "model150.init_sims(replace=True)\n",
    "ensemble_train150 = getAvgFeatureVecs(clean_train_reviews, model150, 300)\n",
    "X_train150 = ensemble_train150.astype('float32')\n",
    "ensemble_test150  = getAvgFeatureVecs(clean_test_reviews, model150, 300)\n",
    "X_test150 = ensemble_test150.astype('float32')\n",
    "\n",
    "gensim_file200='20ensemble_vectors_epoch200.txt'\n",
    "model200 = gensim.models.Word2Vec.load_word2vec_format(gensim_file200, binary=False) \n",
    "model200.init_sims(replace=True)\n",
    "ensemble_train200 = getAvgFeatureVecs(clean_train_reviews, model200, 300)\n",
    "X_train200 = ensemble_train200.astype('float32')\n",
    "ensemble_test200  = getAvgFeatureVecs(clean_test_reviews, model200, 300)\n",
    "X_test200 = ensemble_test200.astype('float32')\n",
    "\n",
    "gensim_file250='20ensemble_vectors_epoch250.txt'\n",
    "model250 = gensim.models.Word2Vec.load_word2vec_format(gensim_file250, binary=False) \n",
    "model250.init_sims(replace=True)\n",
    "ensemble_train250 = getAvgFeatureVecs(clean_train_reviews, model250, 300)\n",
    "X_train250 = ensemble_train250.astype('float32')\n",
    "ensemble_test250  = getAvgFeatureVecs(clean_test_reviews, model250, 300)\n",
    "X_test250 = ensemble_test250.astype('float32')\n",
    "\n",
    "gensim_file300='20ensemble_vectors_epoch300.txt'\n",
    "model300 = gensim.models.Word2Vec.load_word2vec_format(gensim_file300, binary=False) \n",
    "model300.init_sims(replace=True)\n",
    "ensemble_train300 = getAvgFeatureVecs(clean_train_reviews, model300, 300)\n",
    "X_train300 = ensemble_train300.astype('float32')\n",
    "ensemble_test300  = getAvgFeatureVecs(clean_test_reviews, model300, 300)\n",
    "X_test300 = ensemble_test300.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.809994508512\n",
      "0.809994508512\n",
      "0.809445359692\n",
      "0.809445359692\n",
      "0.809445359692\n",
      "0.81109280615\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train50, train_['label'])\n",
    "print clf.score(X_test50, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train100, train_['label'])\n",
    "print clf.score(X_test100, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train150, train_['label'])\n",
    "print clf.score(X_test150, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train200, train_['label'])\n",
    "print clf.score(X_test200, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train250, train_['label'])\n",
    "print clf.score(X_test250, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train300, train_['label'])\n",
    "print clf.score(X_test300, test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "X_valid50  = getAvgFeatureVecs(clean_valid_reviews, model50 , 300)\n",
    "X_valid100 = getAvgFeatureVecs(clean_valid_reviews, model100, 300)\n",
    "X_valid150 = getAvgFeatureVecs(clean_valid_reviews, model150, 300)\n",
    "X_valid200 = getAvgFeatureVecs(clean_valid_reviews, model200, 300)\n",
    "X_valid250 = getAvgFeatureVecs(clean_valid_reviews, model250, 300)\n",
    "X_valid300 = getAvgFeatureVecs(clean_valid_reviews, model300, 300)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(train_['label'])\n",
    "y_valid_ohe = np_utils.to_categorical(valid_['label'])\n",
    "y_test_ohe  = np_utils.to_categorical(test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1754 - acc: 0.7294 - val_loss: 0.1433 - val_acc: 0.7982\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1371 - acc: 0.8050 - val_loss: 0.1369 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1341 - acc: 0.8090 - val_loss: 0.1353 - val_acc: 0.8062\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1334 - acc: 0.8093 - val_loss: 0.1378 - val_acc: 0.8108\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1319 - acc: 0.8122 - val_loss: 0.1344 - val_acc: 0.7982\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1304 - acc: 0.8154 - val_loss: 0.1337 - val_acc: 0.8016\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1294 - acc: 0.8171 - val_loss: 0.1329 - val_acc: 0.8073\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1287 - acc: 0.8179 - val_loss: 0.1322 - val_acc: 0.8005\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1278 - acc: 0.8200 - val_loss: 0.1319 - val_acc: 0.8050\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1274 - acc: 0.8202 - val_loss: 0.1328 - val_acc: 0.8062\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1263 - acc: 0.8218 - val_loss: 0.1336 - val_acc: 0.8096\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1252 - acc: 0.8238 - val_loss: 0.1305 - val_acc: 0.8108\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1248 - acc: 0.8240 - val_loss: 0.1321 - val_acc: 0.8085\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1237 - acc: 0.8259 - val_loss: 0.1359 - val_acc: 0.8039\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1232 - acc: 0.8268 - val_loss: 0.1294 - val_acc: 0.8119\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.806699615596\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train50,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid50,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test50))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1767 - acc: 0.7245 - val_loss: 0.1440 - val_acc: 0.7913\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1363 - acc: 0.8067 - val_loss: 0.1369 - val_acc: 0.7970\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1333 - acc: 0.8099 - val_loss: 0.1357 - val_acc: 0.8039\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1326 - acc: 0.8115 - val_loss: 0.1358 - val_acc: 0.8096\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1315 - acc: 0.8124 - val_loss: 0.1349 - val_acc: 0.7867\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1301 - acc: 0.8161 - val_loss: 0.1336 - val_acc: 0.7982\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1289 - acc: 0.8170 - val_loss: 0.1331 - val_acc: 0.8062\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1281 - acc: 0.8196 - val_loss: 0.1324 - val_acc: 0.7959\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1272 - acc: 0.8217 - val_loss: 0.1322 - val_acc: 0.8039\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1264 - acc: 0.8225 - val_loss: 0.1330 - val_acc: 0.8062\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1251 - acc: 0.8242 - val_loss: 0.1334 - val_acc: 0.8085\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1242 - acc: 0.8254 - val_loss: 0.1310 - val_acc: 0.8039\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1236 - acc: 0.8268 - val_loss: 0.1326 - val_acc: 0.8039\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1226 - acc: 0.8290 - val_loss: 0.1333 - val_acc: 0.8073\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1218 - acc: 0.8302 - val_loss: 0.1306 - val_acc: 0.8073\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.81109280615\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train100,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid100,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test100))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1739 - acc: 0.7336 - val_loss: 0.1436 - val_acc: 0.7947\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1357 - acc: 0.8072 - val_loss: 0.1367 - val_acc: 0.7970\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1328 - acc: 0.8111 - val_loss: 0.1358 - val_acc: 0.8005\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1321 - acc: 0.8119 - val_loss: 0.1358 - val_acc: 0.8096\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1309 - acc: 0.8135 - val_loss: 0.1350 - val_acc: 0.7901\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1295 - acc: 0.8171 - val_loss: 0.1339 - val_acc: 0.7959\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1281 - acc: 0.8187 - val_loss: 0.1334 - val_acc: 0.8085\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1271 - acc: 0.8217 - val_loss: 0.1327 - val_acc: 0.7947\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1261 - acc: 0.8232 - val_loss: 0.1325 - val_acc: 0.8039\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1249 - acc: 0.8247 - val_loss: 0.1344 - val_acc: 0.8096\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1234 - acc: 0.8275 - val_loss: 0.1340 - val_acc: 0.8142\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1219 - acc: 0.8297 - val_loss: 0.1313 - val_acc: 0.8016\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1210 - acc: 0.8317 - val_loss: 0.1327 - val_acc: 0.8108\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1196 - acc: 0.8341 - val_loss: 0.1326 - val_acc: 0.8119\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1184 - acc: 0.8359 - val_loss: 0.1311 - val_acc: 0.8062\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.816035145524\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train150,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid150,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test150))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1718 - acc: 0.7367 - val_loss: 0.1422 - val_acc: 0.7993\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1350 - acc: 0.8073 - val_loss: 0.1363 - val_acc: 0.7982\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1324 - acc: 0.8114 - val_loss: 0.1357 - val_acc: 0.7982\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1317 - acc: 0.8130 - val_loss: 0.1357 - val_acc: 0.8062\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1305 - acc: 0.8138 - val_loss: 0.1351 - val_acc: 0.7947\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1288 - acc: 0.8184 - val_loss: 0.1345 - val_acc: 0.7878\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1276 - acc: 0.8199 - val_loss: 0.1346 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1266 - acc: 0.8217 - val_loss: 0.1344 - val_acc: 0.7970\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1257 - acc: 0.8246 - val_loss: 0.1339 - val_acc: 0.8016\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1246 - acc: 0.8238 - val_loss: 0.1349 - val_acc: 0.8016\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1230 - acc: 0.8269 - val_loss: 0.1340 - val_acc: 0.8085\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1215 - acc: 0.8296 - val_loss: 0.1346 - val_acc: 0.8050\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1205 - acc: 0.8316 - val_loss: 0.1354 - val_acc: 0.7993\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1186 - acc: 0.8352 - val_loss: 0.1351 - val_acc: 0.8062\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1173 - acc: 0.8375 - val_loss: 0.1326 - val_acc: 0.8028\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.813838550247\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train200,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid200,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test200))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1697 - acc: 0.7416 - val_loss: 0.1411 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1345 - acc: 0.8084 - val_loss: 0.1359 - val_acc: 0.8005\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1320 - acc: 0.8121 - val_loss: 0.1354 - val_acc: 0.7982\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1311 - acc: 0.8141 - val_loss: 0.1354 - val_acc: 0.8050\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1296 - acc: 0.8158 - val_loss: 0.1338 - val_acc: 0.7947\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1273 - acc: 0.8204 - val_loss: 0.1326 - val_acc: 0.7947\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1252 - acc: 0.8242 - val_loss: 0.1330 - val_acc: 0.8050\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1232 - acc: 0.8273 - val_loss: 0.1307 - val_acc: 0.7993\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1209 - acc: 0.8328 - val_loss: 0.1307 - val_acc: 0.8028\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1187 - acc: 0.8358 - val_loss: 0.1319 - val_acc: 0.8028\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1156 - acc: 0.8405 - val_loss: 0.1316 - val_acc: 0.8142\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1131 - acc: 0.8449 - val_loss: 0.1288 - val_acc: 0.8085\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1108 - acc: 0.8476 - val_loss: 0.1317 - val_acc: 0.8119\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1079 - acc: 0.8543 - val_loss: 0.1313 - val_acc: 0.8108\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1062 - acc: 0.8573 - val_loss: 0.1298 - val_acc: 0.8188\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.812191103789\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train250,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid250,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test250))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1692 - acc: 0.7443 - val_loss: 0.1410 - val_acc: 0.7924\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1347 - acc: 0.8080 - val_loss: 0.1358 - val_acc: 0.7993\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1319 - acc: 0.8127 - val_loss: 0.1348 - val_acc: 0.7982\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1308 - acc: 0.8147 - val_loss: 0.1347 - val_acc: 0.8073\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1291 - acc: 0.8168 - val_loss: 0.1340 - val_acc: 0.7947\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1266 - acc: 0.8220 - val_loss: 0.1328 - val_acc: 0.7993\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1243 - acc: 0.8256 - val_loss: 0.1327 - val_acc: 0.8039\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1224 - acc: 0.8294 - val_loss: 0.1324 - val_acc: 0.7993\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1204 - acc: 0.8332 - val_loss: 0.1319 - val_acc: 0.8039\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1187 - acc: 0.8356 - val_loss: 0.1327 - val_acc: 0.8131\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1160 - acc: 0.8405 - val_loss: 0.1310 - val_acc: 0.8142\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1140 - acc: 0.8435 - val_loss: 0.1318 - val_acc: 0.8062\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1125 - acc: 0.8468 - val_loss: 0.1342 - val_acc: 0.7982\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1104 - acc: 0.8510 - val_loss: 0.1311 - val_acc: 0.8108\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1087 - acc: 0.8532 - val_loss: 0.1301 - val_acc: 0.8177\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.807248764415\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train300,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid300,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test300))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
