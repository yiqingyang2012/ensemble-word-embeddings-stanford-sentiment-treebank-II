{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, CuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from numpy import random as rng\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "rng = np.random.RandomState(123) \n",
    "theano_rng = RandomStreams( rng.randint(2**30)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame.from_csv('all_train1.csv', index_col=False)\n",
    "valid_ = pd.DataFrame.from_csv('valid.csv'  , index_col=False)\n",
    "test_  = pd.DataFrame.from_csv('test.csv'   , index_col=False)\n",
    "\n",
    "\n",
    "glove_file='corpus_vectors_GloVe_300d.txt'\n",
    "glove=gensim.models.Word2Vec.load_word2vec_format(glove_file,binary=False)\n",
    "\n",
    "w2v_file='corpus_vectors_W2V_300d.txt'\n",
    "w2v=gensim.models.Word2Vec.load_word2vec_format(w2v_file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Denoising_Autoencoder(object):\n",
    "    def __init__(self, X, hidden_size, noise_type, noise_level, activation_function, output_function):\n",
    "        \n",
    "        self.m = X.shape[0]\n",
    "        self.n = X.shape[1] - 1\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "        # The clean data\n",
    "        self.X_clean = X\n",
    "        self.X_clean = theano.shared(name='X_clean', value=np.asarray(self.X_clean, dtype=theano.config.floatX))\n",
    "        \n",
    "        \n",
    "        # The noisy data\n",
    "        X_noisy = np.zeros((self.m, self.n+1))\n",
    "        for i in xrange(self.m):\n",
    "            for j in xrange(self.n + 1):\n",
    "                X_noisy[i][j] = X[i][j]\n",
    "        #X_noisy = X[:,:]\n",
    "        if noise_type == 'salt_peeper_noise':\n",
    "            for i in range(self.m):\n",
    "                random_positions = random.sample(range(0, self.n), int(self.n*noise_level))\n",
    "                for j in range(int(self.n*noise_level)):\n",
    "                    X_noisy[i][random_positions[j]] = float(random.getrandbits(1))\n",
    "        if noise_type == 'mask_noise':\n",
    "            for i in range(self.m):\n",
    "                zero_positions = random.sample(range(0, self.n), int(self.n*noise_level))\n",
    "                for j in range(int(self.n*noise_level)):\n",
    "                    X_noisy[i][zero_positions[j]] = 0.0\n",
    "        self.X_noisy = X_noisy\n",
    "        self.X_noisy = theano.shared(name ='X_noisy', value=np.asarray(self.X_noisy, dtype=theano.config.floatX))\n",
    "        \n",
    "        \n",
    "        \n",
    "        initial_W = np.asarray(rng.uniform(low = -4 * np.sqrt(6./(self.hidden_size + self.n)),\n",
    "                                           high = 4 * np.sqrt(6./(self.hidden_size + self.n)),\n",
    "                                           size = (self.n,self.hidden_size)) , dtype = theano.config.floatX)\n",
    "        self.W  = theano.shared(name='W', value=initial_W)\n",
    "        self.b1 = theano.shared(name='b1',value=np.zeros(shape=(self.hidden_size), dtype=theano.config.floatX))\n",
    "        self.b2 = theano.shared(name='b2',value=np.zeros(shape=(self.n),           dtype=theano.config.floatX))\n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.output_function = output_function\n",
    "        self.index_list = []                    # During shuffling we need to keep track of word's index\n",
    "        self.hidden_list = [] \n",
    "\n",
    "    \n",
    "    def trainSDA(self, loss_function, n_epochs = 100, mini_batch_size = 1, learning_rate = 0.1):\n",
    "        index = T.lscalar()\n",
    "        x_clean = T.matrix('x_clean')\n",
    "        x_noisy = T.matrix('x_noisy')\n",
    "        params = [self.W, self.b1, self.b2]\n",
    "        \n",
    "        \n",
    "        # Forward Propagation\n",
    "        hidden = self.activation_function(T.dot(x_noisy, self.W) + self.b1)\n",
    "        output = self.output_function(T.dot(hidden, T.transpose(self.W)) + self.b2)\n",
    "        \n",
    "        \n",
    "        # Loss Expression\n",
    "        if loss_function == 'cross entropy loss':\n",
    "            loss = -T.sum(x_clean*T.log(output) + (1-x_clean) * T.log(1-output)).mean()\n",
    "        if loss_function == 'sum squared loss':\n",
    "            loss = ((x_clean - output)**2).sum()/(self.m)\n",
    "        \n",
    "        \n",
    "        # Gradient Descent Optimization\n",
    "        #learning_rate = 0.5\n",
    "        #updates = []\n",
    "        #gparams = T.grad(loss,params)\n",
    "        #for param,gparam in zip(params, gparams):\n",
    "        #    updates.append((param, param - learning_rate * gparam))\n",
    "        \n",
    "        \n",
    "        # Adam Optimization\n",
    "        lr=0.0002 \n",
    "        beta1=0.1 \n",
    "        beta2=0.001\n",
    "        epsilon=1e-8 \n",
    "        gamma=1-1e-7\n",
    "        updates = []\n",
    "        grads = theano.grad(loss, params)\n",
    "        i = theano.shared(np.float32(1))  # HOW to init scalar shared?\n",
    "        i_t = i + 1.\n",
    "        fix1 = 1. - (1. - beta1)**i_t\n",
    "        fix2 = 1. - (1. - beta2)**i_t\n",
    "        beta1_t = 1-(1-beta1)*gamma**(i_t-1)   # ADDED\n",
    "        lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "        for param_i, g in zip(params, grads):\n",
    "            m = theano.shared(\n",
    "                np.zeros(param_i.get_value().shape, dtype=theano.config.floatX))\n",
    "            v = theano.shared(\n",
    "                np.zeros(param_i.get_value().shape, dtype=theano.config.floatX))\n",
    "            m_t = (beta1_t * g) + ((1. - beta1_t) * m) # CHANGED from b_t TO use beta1_t\n",
    "            v_t = (beta2 * g**2) + ((1. - beta2) * v)\n",
    "            g_t = m_t / (T.sqrt(v_t) + epsilon)\n",
    "            param_i_t = param_i - (lr_t * g_t)\n",
    "            updates.append((m, m_t))\n",
    "            updates.append((v, v_t))\n",
    "            updates.append((param_i, param_i_t) )\n",
    "        updates.append((i, i_t))\n",
    "        # End of Adam Optimization\n",
    "        \n",
    "        \n",
    "        \n",
    "        train_func = theano.function(inputs=[index], outputs=[loss], updates=updates,\n",
    "                            givens={x_clean:self.X_clean[index:index+mini_batch_size,:-1], \n",
    "                                    x_noisy:self.X_noisy[index:index+mini_batch_size,:-1]})\n",
    "        loss_func = theano.function(inputs=[], outputs=[loss],\n",
    "                                   givens={x_clean:self.X_clean[:,:-1],\n",
    "                                           x_noisy:self.X_noisy[:,:-1]})\n",
    "        \n",
    "        \n",
    "        \n",
    "        import time\n",
    "        start_time = time.clock()\n",
    "        for epoch in xrange(n_epochs):\n",
    "            for row in xrange(0, self.m, mini_batch_size):\n",
    "                train_func(row)\n",
    "            self.shuffleX()\n",
    "            if (epoch+1) % 50 == 0:\n",
    "                print \"Epoch : \",epoch+1\n",
    "                self.index_list.append(self.X_clean[:,-1:].eval())\n",
    "                self.hidden_list.append(self.get_hidden_layer())\n",
    "        end_time = time.clock()\n",
    "        print \"Average training timr per epoch : \", (end_time - start_time)/n_epochs\n",
    "        \n",
    "\n",
    "        \n",
    "    def shuffleX(self):\n",
    "        rng = np.random.RandomState(np.random.randint(0,400)) \n",
    "        theano_rng = RandomStreams(rng.randint(2**20)) \n",
    "        rows = self.m\n",
    "        #Z = np.concatenate((self.X_clean, self.X_noisy), axis=1)\n",
    "        Z = np.concatenate((self.X_clean.eval(), self.X_noisy.eval()), axis=1)\n",
    "        #Z = theano.shared(name='Z', value=Z)\n",
    "        Y = T.permute_row_elements(Z.T, theano_rng.permutation(n=rows)).T.eval()\n",
    "        X_clean_update = (self.X_clean, Y[ :, :self.n+1])\n",
    "        X_noisy_update = (self.X_noisy, Y[ :, self.n+1:])\n",
    "        f1 = theano.function(inputs=[],updates=[X_clean_update])\n",
    "        f2 = theano.function(inputs=[],updates=[X_noisy_update])\n",
    "        f1()\n",
    "        f2()\n",
    "        \n",
    "   \n",
    "    def get_hidden_layer(self):\n",
    "        x = T.matrix('x')\n",
    "        hidden_approximation = self.activation_function(T.dot(x, self.W) + self.b1)\n",
    "        hidden_func = theano.function(inputs=[], outputs = hidden_approximation, givens={x:self.X_clean[:,:-1]})\n",
    "        return hidden_func()\n",
    "    \n",
    "    def get_hidden_all(self):\n",
    "        hidden_all = T.tensor3('hidden_all')\n",
    "        hidden_all = theano.shared(name='hidden_all', value=self.hidden_list)\n",
    "        hidden_all_func = theano.function(inputs=[], outputs = hidden_all)\n",
    "        return hidden_all_func()\n",
    "    \n",
    "    def get_index_all(self):\n",
    "        index_all = T.matrix('index_all')\n",
    "        index_all = theano.shared(name='index_all', value=self.index_list)\n",
    "        index_all_func = theano.function(inputs=[], outputs = index_all)\n",
    "        return index_all_func()\n",
    "\n",
    "    def get_weights1(self):\n",
    "        get_weights_func = theano.function(inputs=[], outputs = [self.W, self.b1])\n",
    "        return get_weights_func()\n",
    "    \n",
    "    def get_weights2(self):\n",
    "        get_weights_func = theano.function(inputs=[], outputs = [T.transpose(self.W), self.b2])\n",
    "        return get_weights_func()\n",
    "    \n",
    "    def get_hidden(self):\n",
    "        x_clean = T.matrix('x_clean')\n",
    "        hidden_approximation = self.activation_function(T.dot(x_clean, self.W) + self.b1)\n",
    "        hidden_func = theano.function(inputs=[], outputs = hidden_approximation, givens={x_clean:self.X_clean})\n",
    "        return hidden_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_modification(raw_review):\n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", raw_review)\n",
    "    lower_case = letters.lower()\n",
    "    words = lower_case.split()\n",
    "    return(words)\n",
    "\n",
    "def corpus_review_modification(raw_review):\n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", raw_review)\n",
    "    lower_case = letters.lower()\n",
    "    words = lower_case.split()\n",
    "    return( \" \".join(words )) \n",
    "\n",
    "\n",
    "def create_denoising_autoencoder(X, hidden_size, noise_type, noise_level, n_epoch = 10, batch_size = 20, \n",
    "                                 loss_function='sum squared loss', activation_function=T.tanh, output_function=T.tanh):\n",
    "    \n",
    "    A = Denoising_Autoencoder(X, hidden_size, noise_type, noise_level, activation_function, output_function)\n",
    "    A.trainSDA(loss_function, n_epoch, batch_size)\n",
    "    hidden_all = A.get_hidden_all()\n",
    "    idx = A.get_index_all()\n",
    "    return hidden_all, idx\n",
    "    \n",
    "# Function to create feature vectors for documents using word-vector averages\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To correct spelling mistakes we can use Peter Norvig's pytohn code\n",
    "import re, collections\n",
    "from nltk.corpus import wordnet \n",
    "class RepeatReplacer(object): \n",
    "    def __init__(self):    \n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')    \n",
    "        self.repl = r'\\1\\2\\3'  \n",
    "    def replace(self, word):    \n",
    "        if wordnet.synsets(word):      \n",
    "            return word    \n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)    \n",
    "        if repl_word != word:      \n",
    "            return self.replace(repl_word)    \n",
    "        else:      \n",
    "            return repl_word\n",
    "        \n",
    "\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "\n",
    "def train_spelling(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "\n",
    "NWORDS = train_spelling(words(file('big.txt').read()))\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def edits1(word):\n",
    "   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "   deletes    = [a + b[1:] for a, b in splits if b]\n",
    "   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "   inserts    = [a + c + b     for a, b in splits for c in alphabet]\n",
    "   return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)\n",
    "replacer = RepeatReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16974\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "clean_train_reviews = []\n",
    "clean_valid_reviews = []\n",
    "clean_test_reviews  = []\n",
    "len_train = train_['sent'].size\n",
    "len_valid = valid_['sent'].size\n",
    "len_test  = test_['sent'].size\n",
    "\n",
    "for i in xrange(len_train):\n",
    "    clean_train_reviews.append(review_modification(train_['sent'][i]))\n",
    "    corpus.append(corpus_review_modification(train_['sent'][i]))\n",
    "for i in xrange(len_valid):\n",
    "    clean_valid_reviews.append(review_modification(valid_['sent'][i]))\n",
    "    corpus.append(corpus_review_modification(valid_['sent'][i]))\n",
    "for i in xrange(len_test):\n",
    "    clean_test_reviews.append(review_modification(test_['sent'][i]))\n",
    "    corpus.append(corpus_review_modification(test_['sent'][i]))\n",
    "    \n",
    "vectorizer = CountVectorizer(analyzer = 'word', tokenizer = None, preprocessor = None, stop_words = None)\n",
    "vectorizer.fit_transform(corpus)\n",
    "vocabulary = vectorizer.vocabulary_\n",
    "index = vocabulary.values()\n",
    "index = np.array(index).reshape((len(vocabulary),1))\n",
    "voc_words = vocabulary.keys()\n",
    "print len(vocabulary) \n",
    "mapping = dict((v,k) for k,v in vocabulary.iteritems())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_vector1 = np.random.randn(16974,300)\n",
    "for i in xrange(16974):\n",
    "    try:\n",
    "        my_vector1[i] = glove[voc_words[i]]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            my_vector1[i] = glove[correct(replacer.replace(voc_words[i]))]\n",
    "        except KeyError:\n",
    "            my_vector1[i] = np.random.uniform(low=-0.25, high=0.25, size=(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vector2 = np.random.randn(16974,300)\n",
    "for i in xrange(16974):\n",
    "    try:\n",
    "        my_vector2[i] = w2v[voc_words[i]]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            my_vector2[i] = w2v[correct(replacer.replace(voc_words[i]))]\n",
    "        except KeyError:\n",
    "            my_vector2[i] = np.random.uniform(low=-0.25, high=0.25, size=(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_vector = np.concatenate((my_vector1, my_vector2, index),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no. of epochs = 300\n",
      "Epoch :  50\n",
      "Epoch :  100\n",
      "Epoch :  150\n",
      "Epoch :  200\n",
      "Epoch :  250\n",
      "Epoch :  300\n",
      "Average training timr per epoch :  2.85564898096\n"
     ]
    }
   ],
   "source": [
    "print 'For no. of epochs = 300'\n",
    "X_hidden, index = create_denoising_autoencoder(my_vector, noise_type = 'mask_noise', noise_level=0.35, \n",
    "                                               hidden_size = 300, n_epoch = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_hidden = np.array(X_hidden)\n",
    "index = np.array(index)\n",
    "index = index.reshape((6, 16974))\n",
    "with open('X_hidden35.data', 'w') as f:\n",
    "    pickle.dump(X_hidden, f)\n",
    "with open('index35.data', 'w') as f:\n",
    "    pickle.dump(index, f)\n",
    "with open('mapping.data', 'w') as f:\n",
    "    pickle.dump(mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('clean_train_reviews.data', 'w') as f:\n",
    "    pickle.dump(clean_train_reviews, f)\n",
    "with open('clean_valid_reviews.data', 'w') as f:\n",
    "    pickle.dump(clean_valid_reviews, f)\n",
    "with open('clean_test_reviews.data', 'w') as f:\n",
    "    pickle.dump(clean_test_reviews, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
