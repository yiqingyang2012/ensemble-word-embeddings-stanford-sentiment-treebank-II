{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, CuDNN not available)\n",
      "c:\\scisoft\\theano\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import theano\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano.tensor as T\n",
    "from pandas import DataFrame\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame.from_csv('all_train1.csv', index_col=False)\n",
    "valid_ = pd.DataFrame.from_csv('valid.csv'  , index_col=False)\n",
    "test_  = pd.DataFrame.from_csv('test.csv'   , index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('X_hidden50.data', 'r') as f:\n",
    "    X_hidden = pickle.load(f)\n",
    "with open('index50.data', 'r') as f:\n",
    "    index = pickle.load(f)\n",
    "with open('mapping.data', 'r') as f:\n",
    "    mapping = pickle.load(f) \n",
    "with open('clean_train_reviews.data', 'r') as f:\n",
    "    clean_train_reviews = pickle.load(f)\n",
    "with open('clean_valid_reviews.data', 'r') as f:\n",
    "    clean_valid_reviews = pickle.load(f)\n",
    "with open('clean_test_reviews.data', 'r') as f:\n",
    "    clean_test_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create feature vectors for documents using word-vector averages\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_voc_50 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_50.append(mapping[index[0][i]])\n",
    "table50 = [item for item in X_hidden[0]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table50, columns=headers)\n",
    "df.insert(0, 'word', new_voc_50)\n",
    "np.savetxt(r'50ensemble_vectors_epoch50.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_100 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_100.append(mapping[index[1][i]])\n",
    "table100 = [item for item in X_hidden[1]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table100, columns=headers)\n",
    "df.insert(0, 'word', new_voc_100)\n",
    "np.savetxt(r'50ensemble_vectors_epoch100.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_150 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_150.append(mapping[index[2][i]])\n",
    "table150 = [item for item in X_hidden[2]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table150, columns=headers)\n",
    "df.insert(0, 'word', new_voc_150)\n",
    "np.savetxt(r'50ensemble_vectors_epoch150.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_200 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_200.append(mapping[index[3][i]])\n",
    "table200 = [item for item in X_hidden[3]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table200, columns=headers)\n",
    "df.insert(0, 'word', new_voc_200)\n",
    "np.savetxt(r'50ensemble_vectors_epoch200.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_250 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_250.append(mapping[index[4][i]])\n",
    "table250 = [item for item in X_hidden[4]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table250, columns=headers)\n",
    "df.insert(0, 'word', new_voc_250)\n",
    "np.savetxt(r'50ensemble_vectors_epoch250.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_300 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_300.append(mapping[index[5][i]])\n",
    "table300 = [item for item in X_hidden[5]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table300, columns=headers)\n",
    "df.insert(0, 'word', new_voc_300)\n",
    "np.savetxt(r'50ensemble_vectors_epoch300.txt', df.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "gensim_file50='50ensemble_vectors_epoch50.txt'\n",
    "model50 = gensim.models.Word2Vec.load_word2vec_format(gensim_file50, binary=False) \n",
    "model50.init_sims(replace=True)\n",
    "ensemble_train50 = getAvgFeatureVecs(clean_train_reviews, model50, 300)\n",
    "X_train50 = ensemble_train50.astype('float32')\n",
    "ensemble_test50  = getAvgFeatureVecs(clean_test_reviews, model50, 300)\n",
    "X_test50 = ensemble_test50.astype('float32')\n",
    "\n",
    "gensim_file100='50ensemble_vectors_epoch100.txt'\n",
    "model100 = gensim.models.Word2Vec.load_word2vec_format(gensim_file100, binary=False) \n",
    "model100.init_sims(replace=True)\n",
    "ensemble_train100 = getAvgFeatureVecs(clean_train_reviews, model100, 300)\n",
    "X_train100 = ensemble_train100.astype('float32')\n",
    "ensemble_test100  = getAvgFeatureVecs(clean_test_reviews, model100, 300)\n",
    "X_test100 = ensemble_test100.astype('float32')\n",
    "\n",
    "gensim_file150='50ensemble_vectors_epoch150.txt'\n",
    "model150 = gensim.models.Word2Vec.load_word2vec_format(gensim_file150, binary=False) \n",
    "model150.init_sims(replace=True)\n",
    "ensemble_train150 = getAvgFeatureVecs(clean_train_reviews, model150, 300)\n",
    "X_train150 = ensemble_train150.astype('float32')\n",
    "ensemble_test150  = getAvgFeatureVecs(clean_test_reviews, model150, 300)\n",
    "X_test150 = ensemble_test150.astype('float32')\n",
    "\n",
    "gensim_file200='50ensemble_vectors_epoch200.txt'\n",
    "model200 = gensim.models.Word2Vec.load_word2vec_format(gensim_file200, binary=False) \n",
    "model200.init_sims(replace=True)\n",
    "ensemble_train200 = getAvgFeatureVecs(clean_train_reviews, model200, 300)\n",
    "X_train200 = ensemble_train200.astype('float32')\n",
    "ensemble_test200  = getAvgFeatureVecs(clean_test_reviews, model200, 300)\n",
    "X_test200 = ensemble_test200.astype('float32')\n",
    "\n",
    "gensim_file250='50ensemble_vectors_epoch250.txt'\n",
    "model250 = gensim.models.Word2Vec.load_word2vec_format(gensim_file250, binary=False) \n",
    "model250.init_sims(replace=True)\n",
    "ensemble_train250 = getAvgFeatureVecs(clean_train_reviews, model250, 300)\n",
    "X_train250 = ensemble_train250.astype('float32')\n",
    "ensemble_test250  = getAvgFeatureVecs(clean_test_reviews, model250, 300)\n",
    "X_test250 = ensemble_test250.astype('float32')\n",
    "\n",
    "gensim_file300='50ensemble_vectors_epoch300.txt'\n",
    "model300 = gensim.models.Word2Vec.load_word2vec_format(gensim_file300, binary=False) \n",
    "model300.init_sims(replace=True)\n",
    "ensemble_train300 = getAvgFeatureVecs(clean_train_reviews, model300, 300)\n",
    "X_train300 = ensemble_train300.astype('float32')\n",
    "ensemble_test300  = getAvgFeatureVecs(clean_test_reviews, model300, 300)\n",
    "X_test300 = ensemble_test300.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810543657331\n",
      "0.810543657331\n",
      "0.807797913234\n",
      "0.810543657331\n",
      "0.81109280615\n",
      "0.809994508512\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train50, train_['label'])\n",
    "print clf.score(X_test50, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train100, train_['label'])\n",
    "print clf.score(X_test100, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train150, train_['label'])\n",
    "print clf.score(X_test150, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train200, train_['label'])\n",
    "print clf.score(X_test200, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train250, train_['label'])\n",
    "print clf.score(X_test250, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train300, train_['label'])\n",
    "print clf.score(X_test300, test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "X_valid50  = getAvgFeatureVecs(clean_valid_reviews, model50 , 300)\n",
    "X_valid100 = getAvgFeatureVecs(clean_valid_reviews, model100, 300)\n",
    "X_valid150 = getAvgFeatureVecs(clean_valid_reviews, model150, 300)\n",
    "X_valid200 = getAvgFeatureVecs(clean_valid_reviews, model200, 300)\n",
    "X_valid250 = getAvgFeatureVecs(clean_valid_reviews, model250, 300)\n",
    "X_valid300 = getAvgFeatureVecs(clean_valid_reviews, model300, 300)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(train_['label'])\n",
    "y_valid_ohe = np_utils.to_categorical(valid_['label'])\n",
    "y_test_ohe  = np_utils.to_categorical(test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1704 - acc: 0.7448 - val_loss: 0.1453 - val_acc: 0.7878\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1367 - acc: 0.8049 - val_loss: 0.1397 - val_acc: 0.7993\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1339 - acc: 0.8105 - val_loss: 0.1385 - val_acc: 0.8005\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1332 - acc: 0.8108 - val_loss: 0.1403 - val_acc: 0.7982\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1321 - acc: 0.8122 - val_loss: 0.1375 - val_acc: 0.7993\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1308 - acc: 0.8158 - val_loss: 0.1362 - val_acc: 0.7993\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1300 - acc: 0.8165 - val_loss: 0.1355 - val_acc: 0.8062\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1291 - acc: 0.8176 - val_loss: 0.1349 - val_acc: 0.8005\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1280 - acc: 0.8191 - val_loss: 0.1339 - val_acc: 0.8062\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1271 - acc: 0.8205 - val_loss: 0.1338 - val_acc: 0.8073\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1255 - acc: 0.8227 - val_loss: 0.1350 - val_acc: 0.8154\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1240 - acc: 0.8254 - val_loss: 0.1316 - val_acc: 0.8085\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1230 - acc: 0.8277 - val_loss: 0.1331 - val_acc: 0.8085\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1211 - acc: 0.8300 - val_loss: 0.1336 - val_acc: 0.8154\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1203 - acc: 0.8325 - val_loss: 0.1298 - val_acc: 0.8177\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.813838550247\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train50,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid50,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test50))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1682 - acc: 0.7485 - val_loss: 0.1437 - val_acc: 0.7901\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1358 - acc: 0.8072 - val_loss: 0.1381 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1327 - acc: 0.8132 - val_loss: 0.1366 - val_acc: 0.7993\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1315 - acc: 0.8142 - val_loss: 0.1367 - val_acc: 0.8005\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1299 - acc: 0.8161 - val_loss: 0.1358 - val_acc: 0.8028\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1281 - acc: 0.8197 - val_loss: 0.1346 - val_acc: 0.7970\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1269 - acc: 0.8205 - val_loss: 0.1354 - val_acc: 0.8096\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1258 - acc: 0.8220 - val_loss: 0.1341 - val_acc: 0.8050\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1246 - acc: 0.8244 - val_loss: 0.1339 - val_acc: 0.8073\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1235 - acc: 0.8254 - val_loss: 0.1342 - val_acc: 0.8073\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1217 - acc: 0.8294 - val_loss: 0.1369 - val_acc: 0.8016\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1200 - acc: 0.8317 - val_loss: 0.1330 - val_acc: 0.8096\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1187 - acc: 0.8347 - val_loss: 0.1339 - val_acc: 0.8119\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1167 - acc: 0.8385 - val_loss: 0.1348 - val_acc: 0.7970\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1154 - acc: 0.8412 - val_loss: 0.1327 - val_acc: 0.8119\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.81164195497\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train100,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid100,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test100))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1678 - acc: 0.7498 - val_loss: 0.1431 - val_acc: 0.7924\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1357 - acc: 0.8077 - val_loss: 0.1386 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1330 - acc: 0.8122 - val_loss: 0.1374 - val_acc: 0.8028\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1317 - acc: 0.8141 - val_loss: 0.1368 - val_acc: 0.8050\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1302 - acc: 0.8159 - val_loss: 0.1361 - val_acc: 0.8016\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1284 - acc: 0.8201 - val_loss: 0.1358 - val_acc: 0.7959\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1272 - acc: 0.8213 - val_loss: 0.1354 - val_acc: 0.7993\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1260 - acc: 0.8231 - val_loss: 0.1355 - val_acc: 0.8016\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1251 - acc: 0.8250 - val_loss: 0.1346 - val_acc: 0.8085\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1240 - acc: 0.8260 - val_loss: 0.1356 - val_acc: 0.8028\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1224 - acc: 0.8291 - val_loss: 0.1338 - val_acc: 0.8096\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1210 - acc: 0.8316 - val_loss: 0.1340 - val_acc: 0.8119\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1201 - acc: 0.8330 - val_loss: 0.1351 - val_acc: 0.8039\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1184 - acc: 0.8363 - val_loss: 0.1339 - val_acc: 0.8119\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1173 - acc: 0.8378 - val_loss: 0.1325 - val_acc: 0.8165\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.810543657331\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train150,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid150,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test150))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1688 - acc: 0.7478 - val_loss: 0.1428 - val_acc: 0.7890\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1356 - acc: 0.8080 - val_loss: 0.1378 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1327 - acc: 0.8122 - val_loss: 0.1370 - val_acc: 0.7993\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1315 - acc: 0.8145 - val_loss: 0.1366 - val_acc: 0.7993\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1299 - acc: 0.8159 - val_loss: 0.1367 - val_acc: 0.7947\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1280 - acc: 0.8197 - val_loss: 0.1361 - val_acc: 0.7947\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1265 - acc: 0.8221 - val_loss: 0.1349 - val_acc: 0.8039\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1249 - acc: 0.8249 - val_loss: 0.1349 - val_acc: 0.8039\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1235 - acc: 0.8276 - val_loss: 0.1346 - val_acc: 0.8073\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1219 - acc: 0.8302 - val_loss: 0.1351 - val_acc: 0.8028\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1202 - acc: 0.8328 - val_loss: 0.1352 - val_acc: 0.8108\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1184 - acc: 0.8355 - val_loss: 0.1327 - val_acc: 0.8096\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1173 - acc: 0.8377 - val_loss: 0.1337 - val_acc: 0.8108\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1156 - acc: 0.8405 - val_loss: 0.1358 - val_acc: 0.8050\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1142 - acc: 0.8440 - val_loss: 0.1333 - val_acc: 0.8142\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.81109280615\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train200,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid200,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test200))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1687 - acc: 0.7490 - val_loss: 0.1426 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1357 - acc: 0.8077 - val_loss: 0.1376 - val_acc: 0.8028\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1330 - acc: 0.8119 - val_loss: 0.1371 - val_acc: 0.8039\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1320 - acc: 0.8133 - val_loss: 0.1369 - val_acc: 0.8096\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1306 - acc: 0.8152 - val_loss: 0.1360 - val_acc: 0.7982\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1289 - acc: 0.8197 - val_loss: 0.1353 - val_acc: 0.7947\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1277 - acc: 0.8206 - val_loss: 0.1354 - val_acc: 0.8050\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1266 - acc: 0.8221 - val_loss: 0.1339 - val_acc: 0.8005\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1255 - acc: 0.8254 - val_loss: 0.1342 - val_acc: 0.7993\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1242 - acc: 0.8265 - val_loss: 0.1336 - val_acc: 0.8085\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1223 - acc: 0.8294 - val_loss: 0.1330 - val_acc: 0.8108\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1207 - acc: 0.8324 - val_loss: 0.1314 - val_acc: 0.8131\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1192 - acc: 0.8350 - val_loss: 0.1312 - val_acc: 0.8028\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1172 - acc: 0.8377 - val_loss: 0.1338 - val_acc: 0.8016\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1157 - acc: 0.8415 - val_loss: 0.1301 - val_acc: 0.8131\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.815485996705\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train250,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid250,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test250))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1715 - acc: 0.7400 - val_loss: 0.1428 - val_acc: 0.7878\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1360 - acc: 0.8069 - val_loss: 0.1372 - val_acc: 0.8039\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1333 - acc: 0.8120 - val_loss: 0.1367 - val_acc: 0.8005\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1323 - acc: 0.8130 - val_loss: 0.1366 - val_acc: 0.8073\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1314 - acc: 0.8137 - val_loss: 0.1364 - val_acc: 0.7982\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1302 - acc: 0.8163 - val_loss: 0.1369 - val_acc: 0.7856\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1299 - acc: 0.8167 - val_loss: 0.1362 - val_acc: 0.8050\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1292 - acc: 0.8166 - val_loss: 0.1362 - val_acc: 0.7947\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1291 - acc: 0.8185 - val_loss: 0.1356 - val_acc: 0.7982\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1285 - acc: 0.8185 - val_loss: 0.1359 - val_acc: 0.8050\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1274 - acc: 0.8197 - val_loss: 0.1345 - val_acc: 0.8062\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1265 - acc: 0.8212 - val_loss: 0.1340 - val_acc: 0.8062\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1262 - acc: 0.8220 - val_loss: 0.1346 - val_acc: 0.8005\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1250 - acc: 0.8236 - val_loss: 0.1336 - val_acc: 0.8039\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1243 - acc: 0.8247 - val_loss: 0.1328 - val_acc: 0.8039\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.813838550247\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train300,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid300,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test300))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
