{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 755M (CNMeM is enabled with initial size: 80.0% of memory, CuDNN not available)\n",
      "c:\\scisoft\\theano\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import theano\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano.tensor as T\n",
    "from pandas import DataFrame\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = pd.DataFrame.from_csv('all_train1.csv', index_col=False)\n",
    "valid_ = pd.DataFrame.from_csv('valid.csv'  , index_col=False)\n",
    "test_  = pd.DataFrame.from_csv('test.csv'   , index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('X_hidden35.data', 'r') as f:\n",
    "    X_hidden = pickle.load(f)\n",
    "with open('index35.data', 'r') as f:\n",
    "    index = pickle.load(f)\n",
    "with open('mapping.data', 'r') as f:\n",
    "    mapping = pickle.load(f) \n",
    "with open('clean_train_reviews.data', 'r') as f:\n",
    "    clean_train_reviews = pickle.load(f)\n",
    "with open('clean_valid_reviews.data', 'r') as f:\n",
    "    clean_valid_reviews = pickle.load(f)\n",
    "with open('clean_test_reviews.data', 'r') as f:\n",
    "    clean_test_reviews = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create feature vectors for documents using word-vector averages\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype='float32')\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_voc_50 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_50.append(mapping[index[0][i]])\n",
    "table50 = [item for item in X_hidden[0]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table50, columns=headers)\n",
    "df.insert(0, 'word', new_voc_50)\n",
    "np.savetxt(r'35ensemble_vectors_epoch50.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_100 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_100.append(mapping[index[1][i]])\n",
    "table100 = [item for item in X_hidden[1]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table100, columns=headers)\n",
    "df.insert(0, 'word', new_voc_100)\n",
    "np.savetxt(r'35ensemble_vectors_epoch100.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_150 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_150.append(mapping[index[2][i]])\n",
    "table150 = [item for item in X_hidden[2]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table150, columns=headers)\n",
    "df.insert(0, 'word', new_voc_150)\n",
    "np.savetxt(r'35ensemble_vectors_epoch150.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_200 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_200.append(mapping[index[3][i]])\n",
    "table200 = [item for item in X_hidden[3]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table200, columns=headers)\n",
    "df.insert(0, 'word', new_voc_200)\n",
    "np.savetxt(r'35ensemble_vectors_epoch200.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_250 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_250.append(mapping[index[4][i]])\n",
    "table250 = [item for item in X_hidden[4]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table250, columns=headers)\n",
    "df.insert(0, 'word', new_voc_250)\n",
    "np.savetxt(r'35ensemble_vectors_epoch250.txt', df.values, fmt='%s')\n",
    "\n",
    "new_voc_300 = []\n",
    "for i in xrange(16974):\n",
    "    new_voc_300.append(mapping[index[5][i]])\n",
    "table300 = [item for item in X_hidden[5]]\n",
    "headers = map(str,range(300))\n",
    "df = DataFrame(table300, columns=headers)\n",
    "df.insert(0, 'word', new_voc_300)\n",
    "np.savetxt(r'35ensemble_vectors_epoch300.txt', df.values, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "gensim_file50='35ensemble_vectors_epoch50.txt'\n",
    "model50 = gensim.models.Word2Vec.load_word2vec_format(gensim_file50, binary=False) \n",
    "model50.init_sims(replace=True)\n",
    "ensemble_train50 = getAvgFeatureVecs(clean_train_reviews, model50, 300)\n",
    "X_train50 = ensemble_train50.astype('float32')\n",
    "ensemble_test50  = getAvgFeatureVecs(clean_test_reviews, model50, 300)\n",
    "X_test50 = ensemble_test50.astype('float32')\n",
    "\n",
    "gensim_file100='35ensemble_vectors_epoch100.txt'\n",
    "model100 = gensim.models.Word2Vec.load_word2vec_format(gensim_file100, binary=False) \n",
    "model100.init_sims(replace=True)\n",
    "ensemble_train100 = getAvgFeatureVecs(clean_train_reviews, model100, 300)\n",
    "X_train100 = ensemble_train100.astype('float32')\n",
    "ensemble_test100  = getAvgFeatureVecs(clean_test_reviews, model100, 300)\n",
    "X_test100 = ensemble_test100.astype('float32')\n",
    "\n",
    "gensim_file150='35ensemble_vectors_epoch150.txt'\n",
    "model150 = gensim.models.Word2Vec.load_word2vec_format(gensim_file150, binary=False) \n",
    "model150.init_sims(replace=True)\n",
    "ensemble_train150 = getAvgFeatureVecs(clean_train_reviews, model150, 300)\n",
    "X_train150 = ensemble_train150.astype('float32')\n",
    "ensemble_test150  = getAvgFeatureVecs(clean_test_reviews, model150, 300)\n",
    "X_test150 = ensemble_test150.astype('float32')\n",
    "\n",
    "gensim_file200='35ensemble_vectors_epoch200.txt'\n",
    "model200 = gensim.models.Word2Vec.load_word2vec_format(gensim_file200, binary=False) \n",
    "model200.init_sims(replace=True)\n",
    "ensemble_train200 = getAvgFeatureVecs(clean_train_reviews, model200, 300)\n",
    "X_train200 = ensemble_train200.astype('float32')\n",
    "ensemble_test200  = getAvgFeatureVecs(clean_test_reviews, model200, 300)\n",
    "X_test200 = ensemble_test200.astype('float32')\n",
    "\n",
    "gensim_file250='35ensemble_vectors_epoch250.txt'\n",
    "model250 = gensim.models.Word2Vec.load_word2vec_format(gensim_file250, binary=False) \n",
    "model250.init_sims(replace=True)\n",
    "ensemble_train250 = getAvgFeatureVecs(clean_train_reviews, model250, 300)\n",
    "X_train250 = ensemble_train250.astype('float32')\n",
    "ensemble_test250  = getAvgFeatureVecs(clean_test_reviews, model250, 300)\n",
    "X_test250 = ensemble_test250.astype('float32')\n",
    "\n",
    "gensim_file300='35ensemble_vectors_epoch300.txt'\n",
    "model300 = gensim.models.Word2Vec.load_word2vec_format(gensim_file300, binary=False) \n",
    "model300.init_sims(replace=True)\n",
    "ensemble_train300 = getAvgFeatureVecs(clean_train_reviews, model300, 300)\n",
    "X_train300 = ensemble_train300.astype('float32')\n",
    "ensemble_test300  = getAvgFeatureVecs(clean_test_reviews, model300, 300)\n",
    "X_test300 = ensemble_test300.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81164195497\n",
      "0.812191103789\n",
      "0.813838550247\n",
      "0.814936847886\n",
      "0.814936847886\n",
      "0.816584294344\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train50, train_['label'])\n",
    "print clf.score(X_test50, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train100, train_['label'])\n",
    "print clf.score(X_test100, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train150, train_['label'])\n",
    "print clf.score(X_test150, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train200, train_['label'])\n",
    "print clf.score(X_test200, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train250, train_['label'])\n",
    "print clf.score(X_test250, test_['label'])\n",
    "\n",
    "clf = svm.LinearSVC(max_iter=10000)\n",
    "clf.fit(X_train300, train_['label'])\n",
    "print clf.score(X_test300, test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Scisoft\\WinPython-64bit-2.7.10.3\\python-2.7.10.amd64\\lib\\site-packages\\ipykernel\\__main__.py:17: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "X_valid50  = getAvgFeatureVecs(clean_valid_reviews, model50 , 300)\n",
    "X_valid100 = getAvgFeatureVecs(clean_valid_reviews, model100, 300)\n",
    "X_valid150 = getAvgFeatureVecs(clean_valid_reviews, model150, 300)\n",
    "X_valid200 = getAvgFeatureVecs(clean_valid_reviews, model200, 300)\n",
    "X_valid250 = getAvgFeatureVecs(clean_valid_reviews, model250, 300)\n",
    "X_valid300 = getAvgFeatureVecs(clean_valid_reviews, model300, 300)\n",
    "\n",
    "y_train_ohe = np_utils.to_categorical(train_['label'])\n",
    "y_valid_ohe = np_utils.to_categorical(valid_['label'])\n",
    "y_test_ohe  = np_utils.to_categorical(test_['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1738 - acc: 0.7343 - val_loss: 0.1449 - val_acc: 0.7890\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1364 - acc: 0.8054 - val_loss: 0.1389 - val_acc: 0.8005\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1336 - acc: 0.8104 - val_loss: 0.1375 - val_acc: 0.8050\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1327 - acc: 0.8121 - val_loss: 0.1382 - val_acc: 0.8028\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1316 - acc: 0.8132 - val_loss: 0.1363 - val_acc: 0.8005\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1304 - acc: 0.8166 - val_loss: 0.1356 - val_acc: 0.7982\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1298 - acc: 0.8171 - val_loss: 0.1352 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1292 - acc: 0.8182 - val_loss: 0.1353 - val_acc: 0.7993\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1285 - acc: 0.8190 - val_loss: 0.1339 - val_acc: 0.8005\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1279 - acc: 0.8193 - val_loss: 0.1341 - val_acc: 0.8073\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1267 - acc: 0.8208 - val_loss: 0.1340 - val_acc: 0.8073\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1257 - acc: 0.8227 - val_loss: 0.1317 - val_acc: 0.8062\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1249 - acc: 0.8249 - val_loss: 0.1330 - val_acc: 0.8016\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1238 - acc: 0.8264 - val_loss: 0.1339 - val_acc: 0.8073\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1230 - acc: 0.8275 - val_loss: 0.1303 - val_acc: 0.8085\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.809994508512\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train50,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid50,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test50))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1697 - acc: 0.7443 - val_loss: 0.1430 - val_acc: 0.7982\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1354 - acc: 0.8074 - val_loss: 0.1375 - val_acc: 0.7970\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1327 - acc: 0.8122 - val_loss: 0.1365 - val_acc: 0.8028\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1318 - acc: 0.8136 - val_loss: 0.1365 - val_acc: 0.8073\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1304 - acc: 0.8154 - val_loss: 0.1353 - val_acc: 0.8028\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1292 - acc: 0.8188 - val_loss: 0.1344 - val_acc: 0.8016\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1279 - acc: 0.8197 - val_loss: 0.1345 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1270 - acc: 0.8218 - val_loss: 0.1328 - val_acc: 0.8016\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1257 - acc: 0.8237 - val_loss: 0.1324 - val_acc: 0.8073\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1245 - acc: 0.8247 - val_loss: 0.1338 - val_acc: 0.8108\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1229 - acc: 0.8275 - val_loss: 0.1355 - val_acc: 0.8131\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1213 - acc: 0.8300 - val_loss: 0.1303 - val_acc: 0.8222\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1200 - acc: 0.8330 - val_loss: 0.1310 - val_acc: 0.8142\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1180 - acc: 0.8369 - val_loss: 0.1331 - val_acc: 0.8119\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1165 - acc: 0.8390 - val_loss: 0.1292 - val_acc: 0.8165\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.808896210873\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train100,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid100,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test100))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1680 - acc: 0.7494 - val_loss: 0.1421 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1348 - acc: 0.8087 - val_loss: 0.1372 - val_acc: 0.8016\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1319 - acc: 0.8138 - val_loss: 0.1357 - val_acc: 0.7982\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1308 - acc: 0.8156 - val_loss: 0.1359 - val_acc: 0.8062\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1294 - acc: 0.8169 - val_loss: 0.1345 - val_acc: 0.7970\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1278 - acc: 0.8210 - val_loss: 0.1343 - val_acc: 0.7959\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1269 - acc: 0.8215 - val_loss: 0.1342 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1261 - acc: 0.8231 - val_loss: 0.1336 - val_acc: 0.8028\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1253 - acc: 0.8247 - val_loss: 0.1327 - val_acc: 0.8085\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1242 - acc: 0.8254 - val_loss: 0.1330 - val_acc: 0.8131\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1226 - acc: 0.8280 - val_loss: 0.1328 - val_acc: 0.8085\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1207 - acc: 0.8318 - val_loss: 0.1308 - val_acc: 0.8154\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1193 - acc: 0.8351 - val_loss: 0.1314 - val_acc: 0.8096\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1168 - acc: 0.8384 - val_loss: 0.1311 - val_acc: 0.8131\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1153 - acc: 0.8425 - val_loss: 0.1292 - val_acc: 0.8165\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.806699615596\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train150,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid150,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test150))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1683 - acc: 0.7474 - val_loss: 0.1419 - val_acc: 0.7936\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1350 - acc: 0.8080 - val_loss: 0.1378 - val_acc: 0.8039\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1326 - acc: 0.8124 - val_loss: 0.1367 - val_acc: 0.8016\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1317 - acc: 0.8142 - val_loss: 0.1364 - val_acc: 0.8028\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1306 - acc: 0.8147 - val_loss: 0.1357 - val_acc: 0.7993\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1291 - acc: 0.8188 - val_loss: 0.1360 - val_acc: 0.7947\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1283 - acc: 0.8196 - val_loss: 0.1355 - val_acc: 0.8028\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1275 - acc: 0.8206 - val_loss: 0.1358 - val_acc: 0.7993\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1273 - acc: 0.8217 - val_loss: 0.1348 - val_acc: 0.7982\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1266 - acc: 0.8216 - val_loss: 0.1351 - val_acc: 0.8073\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1256 - acc: 0.8230 - val_loss: 0.1339 - val_acc: 0.8073\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1244 - acc: 0.8245 - val_loss: 0.1347 - val_acc: 0.8005\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1239 - acc: 0.8258 - val_loss: 0.1350 - val_acc: 0.8062\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1226 - acc: 0.8284 - val_loss: 0.1335 - val_acc: 0.8050\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1217 - acc: 0.8299 - val_loss: 0.1331 - val_acc: 0.8062\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.808896210873\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train200,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid200,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test200))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1680 - acc: 0.7491 - val_loss: 0.1422 - val_acc: 0.7901\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1351 - acc: 0.8076 - val_loss: 0.1377 - val_acc: 0.8005\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1325 - acc: 0.8125 - val_loss: 0.1367 - val_acc: 0.8005\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1313 - acc: 0.8143 - val_loss: 0.1367 - val_acc: 0.8028\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1302 - acc: 0.8155 - val_loss: 0.1358 - val_acc: 0.7959\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1285 - acc: 0.8193 - val_loss: 0.1353 - val_acc: 0.7924\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1273 - acc: 0.8212 - val_loss: 0.1351 - val_acc: 0.8073\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1260 - acc: 0.8236 - val_loss: 0.1342 - val_acc: 0.7959\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1248 - acc: 0.8251 - val_loss: 0.1333 - val_acc: 0.8062\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1234 - acc: 0.8266 - val_loss: 0.1346 - val_acc: 0.8108\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1216 - acc: 0.8299 - val_loss: 0.1343 - val_acc: 0.8108\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1198 - acc: 0.8335 - val_loss: 0.1331 - val_acc: 0.8073\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1183 - acc: 0.8360 - val_loss: 0.1322 - val_acc: 0.8005\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1160 - acc: 0.8402 - val_loss: 0.1331 - val_acc: 0.8165\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1144 - acc: 0.8425 - val_loss: 0.1321 - val_acc: 0.8073\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.812191103789\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train250,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid250,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test250))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60639 samples, validate on 872 samples\n",
      "Epoch 1/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1674 - acc: 0.7512 - val_loss: 0.1421 - val_acc: 0.7901\n",
      "Epoch 2/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1350 - acc: 0.8077 - val_loss: 0.1378 - val_acc: 0.7993\n",
      "Epoch 3/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1325 - acc: 0.8127 - val_loss: 0.1369 - val_acc: 0.7993\n",
      "Epoch 4/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1315 - acc: 0.8139 - val_loss: 0.1366 - val_acc: 0.8016\n",
      "Epoch 5/15\n",
      "60639/60639 [==============================] - 0s - loss: 0.1302 - acc: 0.8154 - val_loss: 0.1350 - val_acc: 0.7982\n",
      "Epoch 6/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1282 - acc: 0.8198 - val_loss: 0.1346 - val_acc: 0.7970\n",
      "Epoch 7/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1265 - acc: 0.8223 - val_loss: 0.1343 - val_acc: 0.8073\n",
      "Epoch 8/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1251 - acc: 0.8253 - val_loss: 0.1335 - val_acc: 0.7982\n",
      "Epoch 9/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1238 - acc: 0.8279 - val_loss: 0.1332 - val_acc: 0.8073\n",
      "Epoch 10/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1223 - acc: 0.8285 - val_loss: 0.1335 - val_acc: 0.8085\n",
      "Epoch 11/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1206 - acc: 0.8323 - val_loss: 0.1340 - val_acc: 0.8119\n",
      "Epoch 12/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1188 - acc: 0.8352 - val_loss: 0.1328 - val_acc: 0.8119\n",
      "Epoch 13/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1171 - acc: 0.8386 - val_loss: 0.1329 - val_acc: 0.8073\n",
      "Epoch 14/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1153 - acc: 0.8418 - val_loss: 0.1337 - val_acc: 0.8096\n",
      "Epoch 15/15\n",
      "60639/60639 [==============================] - 1s - loss: 0.1136 - acc: 0.8449 - val_loss: 0.1319 - val_acc: 0.8119\n",
      "1821/1821 [==============================] - 0s     \n",
      "0.806699615596\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(input_dim=300, output_dim=60, init='uniform', activation='relu'))\n",
    "model1.add(Dense(output_dim=30, init='uniform', activation='relu'))\n",
    "model1.add(Dense(input_dim =30, output_dim=y_train_ohe.shape[1], init='uniform', activation='softmax'))\n",
    "model1.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "model1.fit(X_train300,y_train_ohe,nb_epoch=15,batch_size=200,verbose=1,show_accuracy=True,validation_data=[X_valid300,y_valid_ohe])\n",
    "print np.sum(test_['label'] == model1.predict_classes(X_test300))/float(len(test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
